{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "RxK0qC-Q6YqO"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# sys.path.append('../')\n",
        "from data.utils import reformat\n",
        "from icd_classifier.settings import MIMIC_3_DIR, DATA_DIR\n",
        "from data import extract_wvs\n",
        "from data import get_discharge_summaries\n",
        "from data import build_vocab\n",
        "from data import vocab_index_descriptions\n",
        "from data import word_embeddings\n",
        "from data import concat_and_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import csv\n",
        "import math\n",
        "import operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnFT42xP63Mw"
      },
      "source": [
        "# Shared Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/modestas.filipavicius/dev/icd-classifier/icd_classifier\n",
            "/home/modestas.filipavicius/dev/icd-classifier\n",
            "data/raw/\n"
          ]
        }
      ],
      "source": [
        "# sys and python path magic\n",
        "print(os.getcwd())\n",
        "corrected_path = os.path.split(os.getcwd())[0]\n",
        "os.chdir(corrected_path)\n",
        "print(os.getcwd())\n",
        "\n",
        "print(MIMIC_3_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8BB0EHj6Yq5"
      },
      "source": [
        "Let's do some data processing in a much better way, with a notebook.\n",
        "\n",
        "First, let's define some stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "RMrl6j8H6Yq8"
      },
      "outputs": [],
      "source": [
        "Y = 'full'  # use all available labels in the dataset for prediction\n",
        "vocab_size = 'full'  # don't limit the vocab size to a specific number\n",
        "vocab_min = 3  # discard tokens appearing in fewer than this many documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6nxnQgA6Yq9"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD4ITCY16Yq9"
      },
      "source": [
        "## Combine diagnosis and procedure codes and reformat them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9BBiJh16Yq9"
      },
      "source": [
        "The codes in MIMIC-III are given in separate files for procedures and diagnoses, and the codes are given without periods, which might lead to collisions if we naively combine them. So we have to add the periods back in the right place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv.gz' % MIMIC_3_DIR)\n",
        "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv.gz' % MIMIC_3_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "vRYs7egY6YrA"
      },
      "outputs": [],
      "source": [
        "dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(reformat(str(row[4]), True)), axis=1)\n",
        "dfproc['absolute_code'] = dfproc.apply(lambda row: str(reformat(str(row[4]), False)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "TKA6NAAp6YrB"
      },
      "outputs": [],
      "source": [
        "dfcodes = pd.concat([dfdiag, dfproc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROW_ID</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>SEQ_NUM</th>\n",
              "      <th>ICD9_CODE</th>\n",
              "      <th>absolute_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1297</td>\n",
              "      <td>109</td>\n",
              "      <td>172335</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40301</td>\n",
              "      <td>403.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1298</td>\n",
              "      <td>109</td>\n",
              "      <td>172335</td>\n",
              "      <td>2.0</td>\n",
              "      <td>486</td>\n",
              "      <td>486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1299</td>\n",
              "      <td>109</td>\n",
              "      <td>172335</td>\n",
              "      <td>3.0</td>\n",
              "      <td>58281</td>\n",
              "      <td>582.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1300</td>\n",
              "      <td>109</td>\n",
              "      <td>172335</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5855</td>\n",
              "      <td>585.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1301</td>\n",
              "      <td>109</td>\n",
              "      <td>172335</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4254</td>\n",
              "      <td>425.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240090</th>\n",
              "      <td>228330</td>\n",
              "      <td>67415</td>\n",
              "      <td>150871</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3736</td>\n",
              "      <td>37.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240091</th>\n",
              "      <td>228331</td>\n",
              "      <td>67415</td>\n",
              "      <td>150871</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3893</td>\n",
              "      <td>38.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240092</th>\n",
              "      <td>228332</td>\n",
              "      <td>67415</td>\n",
              "      <td>150871</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8872</td>\n",
              "      <td>88.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240093</th>\n",
              "      <td>228333</td>\n",
              "      <td>67415</td>\n",
              "      <td>150871</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3893</td>\n",
              "      <td>38.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240094</th>\n",
              "      <td>228334</td>\n",
              "      <td>67415</td>\n",
              "      <td>150871</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3722</td>\n",
              "      <td>37.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891142 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE absolute_code\n",
              "0         1297         109   172335      1.0     40301        403.01\n",
              "1         1298         109   172335      2.0       486           486\n",
              "2         1299         109   172335      3.0     58281        582.81\n",
              "3         1300         109   172335      4.0      5855         585.5\n",
              "4         1301         109   172335      5.0      4254         425.4\n",
              "...        ...         ...      ...      ...       ...           ...\n",
              "240090  228330       67415   150871      5.0      3736         37.36\n",
              "240091  228331       67415   150871      6.0      3893         38.93\n",
              "240092  228332       67415   150871      7.0      8872         88.72\n",
              "240093  228333       67415   150871      8.0      3893         38.93\n",
              "240094  228334       67415   150871      9.0      3722         37.22\n",
              "\n",
              "[891142 rows x 6 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfcodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use below if COLUMNS ARE in LOWER CASE!\n",
        "dfcodes.to_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, index=False,\n",
        "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
        "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])\n",
        "\n",
        "# dfcodes.to_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, index=False,\n",
        "#                columns=['row_id', 'subject_id', 'hadm_id', 'seq_num', 'absolute_code'],\n",
        "#                header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdu8UTdn6Yrr"
      },
      "source": [
        "## How many codes are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4bXi0WO6Yru",
        "outputId": "5c78ab79-5014-4e53-b28e-561a7909daf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8994"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In the full dataset (not just discharge summaries)\n",
        "df = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
        "# 8994 unique codes\n",
        "len(df['ICD9_CODE'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ICD9_CODE\n",
              "401.9        20703\n",
              "38.93        14731\n",
              "428.0        13111\n",
              "427.31       12891\n",
              "414.01       12429\n",
              "96.04        10333\n",
              "96.6          9300\n",
              "584.9         9119\n",
              "96.71         9100\n",
              "250.00        9058\n",
              "272.4         8690\n",
              "518.81        7497\n",
              "99.04         7244\n",
              "39.61         6838\n",
              "599.0         6555\n",
              "530.81        6326\n",
              "96.72         6048\n",
              "272.0         5930\n",
              "99.55         5842\n",
              "V05.3         5779\n",
              "dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "codes_df = pd.DataFrame(df['ICD9_CODE'])\n",
        "codes_df.value_counts()[:20]\n",
        "# TODO: find a way to plot Zipfs law\n",
        "# ICD9_CODE\n",
        "# 401.9        20703\n",
        "# 38.93        14731\n",
        "# 428.0        13111\n",
        "# 427.31       12891\n",
        "# 414.01       12429\n",
        "# 96.04        10333\n",
        "# 96.6          9300\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PUWV6Z6Yrv"
      },
      "source": [
        "## Tokenize and preprocess raw text --> `disch_full.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veisk8__6Yrw"
      },
      "source": [
        "Preprocessing time!\n",
        "\n",
        "This will:\n",
        "- Select only discharge summaries and their addenda\n",
        "- remove punctuation and numeric-only tokens, removing 500 but keeping 250mg\n",
        "- lowercase all tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "notes_file = \"%s/NOTEEVENTS.csv.gz\" % MIMIC_3_DIR\n",
        "out_file = \"%s/disch_full.csv\" % MIMIC_3_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDha-98t6Yrx",
        "outputId": "81153584-243a-448d-860d-ae8227dd1a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing notes file\n",
            "writing to data/raw//disch_full.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2083180it [01:14, 27924.04it/s]\n"
          ]
        }
      ],
      "source": [
        "# This reads all notes, selects only the discharge summaries, and tokenizes them, returning the output filename\n",
        "disch_full_file = get_discharge_summaries.write_discharge_summaries(notes_file, out_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2A5-Jtl6Yrx"
      },
      "source": [
        "Let's read this in and see what kind of data we're working with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "rRBLgjYA6Yry"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('%s/disch_full.csv' % MIMIC_3_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891142 entries, 0 to 891141\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count   Dtype  \n",
            "---  ------      --------------   -----  \n",
            " 0   ROW_ID      891142 non-null  int64  \n",
            " 1   SUBJECT_ID  891142 non-null  int64  \n",
            " 2   HADM_ID     891142 non-null  int64  \n",
            " 3   SEQ_NUM     891095 non-null  float64\n",
            " 4   ICD9_CODE   891095 non-null  object \n",
            "dtypes: float64(1), int64(3), object(1)\n",
            "memory usage: 34.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBsQSu4-6Yrz",
        "outputId": "1505332a-62c2-4117-aa00-cccd32127cd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "58976"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How many admissions?\n",
        "# 58,976\n",
        "len(df['HADM_ID'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "wTQdw9Rw6Yr0"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m num_tok \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mitertuples():\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m row[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39;49msplit():\n\u001b[1;32m      7\u001b[0m         unique_words\u001b[39m.\u001b[39madd(word)\n\u001b[1;32m      8\u001b[0m         num_tok \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "# Total word tokens and unique words from df['TEXT']\n",
        "# unique_words: {'hemodynacmially', 'walkway', 'costaphrenic', .., 'lately', 'constipaiton', 'excoriate'}\n",
        "unique_words = set()\n",
        "num_tok = 0\n",
        "for row in df.itertuples():\n",
        "    for word in row[4].split():\n",
        "        unique_words.add(word)\n",
        "        num_tok += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJRIt5Ax6Ysc",
        "outputId": "a1fe56e6-4840-4e15-bdee-3ba8ae6a2cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num types 150854\n",
            "Num tokens 79801387\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum types\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(types))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(num_tok))\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtypes example: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlist\u001b[39;49m(num_tok)[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:])\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ],
      "source": [
        "# Num unique words 150,854\n",
        "# Num word tokens 79,801,387\n",
        "print(\"Num unique words: \", len(unique_words))\n",
        "print(\"Num word tokens: \", str(num_tok))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "id": "BjBBnokl6Ysd"
      },
      "outputs": [],
      "source": [
        "# Let's sort by SUBJECT_ID and HADM_ID to make a correspondence\n",
        "# with the MIMIC-3 label file\n",
        "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4tDxreQ6Ysd",
        "outputId": "b2bfe18b-da7e-4708-b068-549c8f0b2c78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_412087/3260084418.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfl = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR)\n"
          ]
        }
      ],
      "source": [
        "# Sort the label file by the same\n",
        "dfl = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR)\n",
        "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yix2g_06Yse",
        "outputId": "abbaab19-359d-4b02-e266-f748a14c5a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(52726, 58976)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df['HADM_ID'].unique()), len(dfl['HADM_ID'].unique())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hC31T4tL6Ysf"
      },
      "source": [
        "## Consolidate labels with set of discharge summaries --> `ALL_CODES_filtered.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0chTOUE6Ysh"
      },
      "source": [
        "Looks like there were some HADM_ID's that didn't have discharge summaries, so they weren't included with our notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "id": "rTORP8WY6Ysi"
      },
      "outputs": [],
      "source": [
        "#Let's filter out these HADM_ID's\n",
        "hadm_ids = set(df['HADM_ID'])\n",
        "with open('%s/ALL_CODES.csv' % MIMIC_3_DIR, 'r') as lf:\n",
        "    with open('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, 'w') as of:\n",
        "        w = csv.writer(of)\n",
        "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
        "        r = csv.reader(lf)\n",
        "        #header\n",
        "        next(r)\n",
        "        for i,row in enumerate(r):\n",
        "            hadm_id = int(row[2])\n",
        "            #print(hadm_id)\n",
        "            #break\n",
        "            if hadm_id in hadm_ids:\n",
        "                w.writerow(row[1:3] + [row[-1], '', ''])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SDg-Rlr6Ysj",
        "outputId": "f1e212b4-3c84-41f4-db61-3992cd7d7068"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_412087/1742194954.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index_col=None)\n"
          ]
        }
      ],
      "source": [
        "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2mtdfNL6Ysj",
        "outputId": "8e37ef04-0b25-412c-9365-8a3bc5666f37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52726"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dfl['HADM_ID'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "P4znzYii6Ysj"
      },
      "outputs": [],
      "source": [
        "# we still need to sort it by HADM_ID\n",
        "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
        "dfl.to_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtmtdwi46Ysj"
      },
      "source": [
        "## Append labels to notes in a single file --> `notes_labeled.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "hU-eYFRx6Ysj"
      },
      "outputs": [],
      "source": [
        "# Now let's append each instance with all of its codes\n",
        "# this is pretty non-trivial so let's use this script I wrote, which requires the notes to be written to file\n",
        "sorted_file = '%s/disch_full.csv' % MIMIC_3_DIR\n",
        "outfilename = '%s/notes_labeled.csv' % MIMIC_3_DIR\n",
        "df.to_csv(sorted_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JU-KQtA6Ysk",
        "outputId": "585805d4-524c-41ad-a84c-38f70a6ff0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CONCATENATING\n",
            "0 done\n",
            "10000 done\n",
            "20000 done\n",
            "30000 done\n",
            "40000 done\n",
            "50000 done\n"
          ]
        }
      ],
      "source": [
        "# CONCATENATING\n",
        "# 0 done\n",
        "# couldn't find matching hadm_id. data is probably not sorted correctly\n",
        "labeled = concat_and_split.concat_data('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, sorted_file, outfilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75euHb-q6YtN",
        "outputId": "b6d69ec8-53b1-4238-f417-dd4345bbe13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mimicdata/mimic3/notes_labeled.csv\n"
          ]
        }
      ],
      "source": [
        "# name of the file we just made\n",
        "print(labeled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWN4fWJ6YtQ"
      },
      "source": [
        "Let's sanity check the combined data we just made. Do we have all hadm id's accounted for, and the same vocab stats?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "UI77jL7C6YtS"
      },
      "outputs": [],
      "source": [
        "dfnl = pd.read_csv(labeled)\n",
        "#  Tokens and unique_words\n",
        "unique_words = set()\n",
        "num_tok = 0\n",
        "for row in dfnl.itertuples():\n",
        "    for w in row[3].split():\n",
        "        unique_words.add(w)\n",
        "        num_tok += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XPS_hP-6YtT",
        "outputId": "80f7c12f-5b93-4d92-915b-b109e56e0c9f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'unique_words' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnum unique_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(unique_words), \u001b[39m\"\u001b[39m\u001b[39mnum tokens\u001b[39m\u001b[39m\"\u001b[39m, num_tok)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unique_words' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"num unique_words\", len(unique_words), \"num tokens\", num_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW1Rgooe6YtT",
        "outputId": "f7093ed5-9fe0-437b-bf29-1416ebe70497"
      },
      "outputs": [],
      "source": [
        "len(dfnl['HADM_ID'].unique())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_t4j85lW6YtU"
      },
      "source": [
        "## Create train/dev/test splits --> `disch_train/dev/test_split.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4EhyF1h6YtU",
        "outputId": "74e91f30-b70a-442a-b46c-c6c756287f50",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SPLITTING\n",
            "0 read\n",
            "10000 read\n",
            "20000 read\n",
            "30000 read\n",
            "40000 read\n",
            "50000 read\n"
          ]
        }
      ],
      "source": [
        "fname = '%s/notes_labeled.csv' % MIMIC_3_DIR\n",
        "base_name = \"%s/disch\" % MIMIC_3_DIR # for output\n",
        "tr, dv, te = concat_and_split.split_data(fname, base_name=base_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Wv9a_26YtU"
      },
      "source": [
        "## Build vocabulary from training data --> `vocab.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3PPTDP_6YtV",
        "outputId": "b28c87a8-1439-49b2-9e99-9894e71568ec",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in data...\n",
            "removing rare terms\n",
            "51917 terms qualify out of 140795 total\n",
            "writing output\n"
          ]
        }
      ],
      "source": [
        "# after building, 51917 words qualify out of 140795 total\n",
        "vocab_min = 3\n",
        "vname = '%s/vocab.csv' % MIMIC_3_DIR\n",
        "build_vocab.build_vocab(vocab_min, tr, vname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "3MlZZLl96YtV"
      },
      "source": [
        "## Sort each data split by length for batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "nR4K1RUH6YtV"
      },
      "outputs": [],
      "source": [
        "for splt in ['train', 'dev', 'test']:\n",
        "    filename = '%s/disch_%s_split.csv' % (MIMIC_3_DIR, splt)\n",
        "    df = pd.read_csv(filename)\n",
        "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
        "    df = df.sort_values(['length'])\n",
        "    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR, splt), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeEsL7L06YtV"
      },
      "source": [
        "## Pre-train word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaMy5xy6YtW"
      },
      "source": [
        "Let's train word embeddings on all words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANZac6NU6YtX",
        "outputId": "d0b45d1f-f480-41bd-fa86-e33ba4a13654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "building word2vec vocab on mimicdata/mimic3/disch_full.csv...\n",
            "training...\n",
            "writing embeddings to mimicdata/mimic3/processed_full.w2v\n"
          ]
        }
      ],
      "source": [
        "w2v_file = word_embeddings.word_embeddings('full', '%s/disch_full.csv' % MIMIC_3_DIR, 100, 0, 5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WOVyIA5Z6YtX"
      },
      "source": [
        "## Write pre-trained word embeddings with new vocab --> `processed_full.embed`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DVO6eVwF6YtX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 51917/51917 [00:00<00:00, 173590.04it/s]\n"
          ]
        }
      ],
      "source": [
        "extract_wvs.gensim_to_embeddings('%s/processed_full.w2v' % MIMIC_3_DIR, '%s/vocab.csv' % MIMIC_3_DIR, Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4rEvFr-n6YuB"
      },
      "source": [
        "## Pre-process code descriptions using the vocab --> ERROR MISSING!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6SUpuims6YuD",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mimicdata/D_ICD_DIAGNOSES.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab_index_descriptions\u001b[39m.\u001b[39;49mvocab_index_descriptions(\u001b[39m'\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/vocab.csv\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m MIMIC_3_DIR,\n\u001b[1;32m      2\u001b[0m                                                   \u001b[39m'\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/description_vectors.vocab\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m MIMIC_3_DIR)\n",
            "File \u001b[0;32m~/dev/caml-mimic/notebooks/../dataproc/vocab_index_descriptions.py:20\u001b[0m, in \u001b[0;36mvocab_index_descriptions\u001b[0;34m(vocab_file, vectors_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m ind2w \u001b[39m=\u001b[39m {i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m: w \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39msorted\u001b[39m(vocab))}\n\u001b[1;32m     19\u001b[0m w2ind \u001b[39m=\u001b[39m {w: i \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m ind2w\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> 20\u001b[0m desc_dict \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_code_descriptions()\n\u001b[1;32m     22\u001b[0m tokenizer \u001b[39m=\u001b[39m RegexpTokenizer(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(vectors_file, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m of:\n",
            "File \u001b[0;32m~/dev/caml-mimic/notebooks/../datasets.py:236\u001b[0m, in \u001b[0;36mload_code_descriptions\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m    234\u001b[0m             desc_dict[\u001b[39mstr\u001b[39m(row[\u001b[39m1\u001b[39m])] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m2\u001b[39m])\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/D_ICD_DIAGNOSES.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (DATA_DIR), \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m descfile:\n\u001b[1;32m    237\u001b[0m         r \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(descfile)\n\u001b[1;32m    238\u001b[0m         \u001b[39m#header\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mimicdata/D_ICD_DIAGNOSES.csv'"
          ]
        }
      ],
      "source": [
        "vocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n",
        "                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2q7E86lk6YuF"
      },
      "source": [
        "## Filter each split to the top 50 diagnosis/procedure codes --> `train/dev/test_50.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "iaV3lhUt6YuG"
      },
      "outputs": [],
      "source": [
        "Y = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "iTd0SiSs6YuG"
      },
      "outputs": [],
      "source": [
        "# first calculate the top k\n",
        "counts = Counter()\n",
        "dfnl = pd.read_csv('%s/notes_labeled.csv' % MIMIC_3_DIR)\n",
        "for row in dfnl.itertuples():\n",
        "    for label in str(row[4]).split(';'):\n",
        "        counts[label] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('401.9', 20053),\n",
              " ('38.93', 14444),\n",
              " ('428.0', 12842),\n",
              " ('427.31', 12594),\n",
              " ('414.01', 12179),\n",
              " ('96.04', 9932),\n",
              " ('96.6', 9161),\n",
              " ('584.9', 8907),\n",
              " ('250.00', 8784),\n",
              " ('96.71', 8619),\n",
              " ('272.4', 8504),\n",
              " ('518.81', 7249),\n",
              " ('99.04', 7147),\n",
              " ('39.61', 6809),\n",
              " ('599.0', 6442),\n",
              " ('530.81', 6156),\n",
              " ('96.72', 5926),\n",
              " ('272.0', 5766),\n",
              " ('285.9', 5296),\n",
              " ('88.56', 5240)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts.most_common()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "IzsHfUsd6YuI"
      },
      "outputs": [],
      "source": [
        "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "7OGXtivD6YuI"
      },
      "outputs": [],
      "source": [
        "codes_50 = [code[0] for code in codes_50[:Y]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OCGS4MG76YuI",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['401.9',\n",
              " '38.93',\n",
              " '428.0',\n",
              " '427.31',\n",
              " '414.01',\n",
              " '96.04',\n",
              " '96.6',\n",
              " '584.9',\n",
              " '250.00',\n",
              " '96.71',\n",
              " '272.4',\n",
              " '518.81',\n",
              " '99.04',\n",
              " '39.61',\n",
              " '599.0',\n",
              " '530.81',\n",
              " '96.72',\n",
              " '272.0',\n",
              " '285.9',\n",
              " '88.56',\n",
              " '244.9',\n",
              " '486',\n",
              " '38.91',\n",
              " '285.1',\n",
              " '36.15',\n",
              " '276.2',\n",
              " '496',\n",
              " '99.15',\n",
              " '995.92',\n",
              " 'V58.61',\n",
              " '507.0',\n",
              " '038.9',\n",
              " '88.72',\n",
              " '585.9',\n",
              " '403.90',\n",
              " '311',\n",
              " '305.1',\n",
              " '37.22',\n",
              " '412',\n",
              " '33.24',\n",
              " '39.95',\n",
              " '287.5',\n",
              " '410.71',\n",
              " '276.1',\n",
              " 'V45.81',\n",
              " '424.0',\n",
              " '45.13',\n",
              " 'V15.82',\n",
              " '511.9',\n",
              " '37.23']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "codes_50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "3WF5HBZI6YuI"
      },
      "outputs": [],
      "source": [
        "with open('%s/TOP_%s_CODES.csv' % (MIMIC_3_DIR, str(Y)), 'w') as of:\n",
        "    w = csv.writer(of)\n",
        "    for code in codes_50:\n",
        "        w.writerow([code])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KLIkR1of6YuJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "dev\n",
            "test\n"
          ]
        }
      ],
      "source": [
        "for splt in ['train', 'dev', 'test']:\n",
        "    print(splt)\n",
        "    hadm_ids = set()\n",
        "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
        "        for line in f:\n",
        "            hadm_ids.add(line.rstrip())\n",
        "    with open('%s/notes_labeled.csv' % MIMIC_3_DIR, 'r') as f:\n",
        "        with open('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), 'w') as of:\n",
        "            r = csv.reader(f)\n",
        "            w = csv.writer(of)\n",
        "            # header\n",
        "            w.writerow(next(r))\n",
        "            i = 0\n",
        "            for row in r:\n",
        "                hadm_id = row[1]\n",
        "                if hadm_id not in hadm_ids:\n",
        "                    continue\n",
        "                codes = set(str(row[3]).split(';'))\n",
        "                filtered_codes = codes.intersection(set(codes_50))\n",
        "                if len(filtered_codes) > 0:\n",
        "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
        "                    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "X7aqxf816YuJ"
      },
      "outputs": [],
      "source": [
        "for splt in ['train', 'dev', 'test']:\n",
        "    filename = '%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y))\n",
        "    df = pd.read_csv(filename)\n",
        "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
        "    df = df.sort_values(['length'])\n",
        "    df.to_csv('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgz5GNOUI3Le"
      },
      "source": [
        "## Done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "dataproc_mimic_III.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0824478ec0a6cd2d31a1829a4ae69205a29f77442e3ba31396463a26e9af8237"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
